# Default values for featbit.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

global:
  imageRegistry: ""
  ## E.g.
  ## imagePullSecrets:
  ##   - myRegistryKeySecretName
  ##
  imagePullSecrets: [ ]
  ingressClassName: "nginx"


nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: { }
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

apiExternalUrl: "http://localhost:5000"
evaluationServerExternalUrl: "http://localhost:5100"
demoExternalUrl: ""
# only considered in exposing LoadBalancer service
autoDiscovery: false

ui:
  replicaCount: 1

  image:
    registry: docker.io
    repository: featbit/featbit-ui
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: 4.2.1

  imagePullSecrets: [ ]

  podAnnotations: { }
  podLabels: { }

  podSecurityContext: { }
  # fsGroup: 2000

  securityContext: { }
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

  service:
    type: ClusterIP
    port: 8081
    nodePort: 30025
    annotations: { }
    staticIP:

  ingress:
    enabled: false
    annotations: { }
    tls:
      enabled: false
      secretName: ""
    path: "/"
    pathType: "ImplementationSpecific"
    host: ""

  rollout:
    # The max surge in pods during a rollout
    maxSurge: 25%
    # The max unavailable during a rollout
    maxUnavailable: 25%

  resources:
    requests: { }
    # cpu: 250m
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 60

  nodeSelector: { }

  tolerations: [ ]

  affinity: { }

  # -- Additional env variables to inject into the da server deployment.
  env: []

api:
  replicaCount: 1

  ssoEnabled: false

  image:
    registry: docker.io
    repository: featbit/featbit-api-server
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: 4.2.1

  imagePullSecrets: [ ]

  podAnnotations: { }
  podLabels: { }

  podSecurityContext: { }
  # fsGroup: 2000

  securityContext: { }
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

  service:
    type: ClusterIP
    port: 5000
    nodePort: 30050
    annotations: { }
    staticIP:

  ingress:
    enabled: false
    annotations: { }
    tls:
      enabled: false
      secretName: ""
    path: "/"
    pathType: "ImplementationSpecific"
    host: ""

  rollout:
    # The max surge in pods during a rollout
    maxSurge: 25%
    # The max unavailable during a rollout
    maxUnavailable: 25%

  resources:
    requests: { }
    # cpu: 250m
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 60

  nodeSelector: { }

  tolerations: [ ]

  affinity: { }

  # -- Additional env variables to inject into the da server deployment.
  env: []

els:
  replicaCount: 1

  image:
    registry: docker.io
    repository: featbit/featbit-evaluation-server
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: 4.2.1

  imagePullSecrets: [ ]

  podAnnotations: { }
  podLabels: { }

  podSecurityContext: { }
  # fsGroup: 2000

  securityContext: { }
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

  service:
    type: ClusterIP
    port: 5100
    nodePort: 30100
    annotations: { }
    staticIP:

  ingress:
    enabled: false
    annotations: { }
    tls:
      enabled: false
      secretName: ""
    path: "/"
    pathType: "ImplementationSpecific"
    host: ""

  rollout:
    # The max surge in pods during a rollout
    maxSurge: 25%
    # The max unavailable during a rollout
    maxUnavailable: 25%

  resources:
    requests: { }
    #  cpu: 250m
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 60

  nodeSelector: { }

  tolerations: [ ]

  affinity: { }

  # -- Additional env variables to inject into the da server deployment.
  env: []


das:
  replicaCount: 1

  image:
    registry: docker.io
    repository: featbit/featbit-data-analytics-server
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: 4.2.1

  imagePullSecrets: [ ]

  podAnnotations: { }
  podLabels: { }

  podSecurityContext: { }
  # fsGroup: 2000

  securityContext: { }
    # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

  service:
    type: ClusterIP
    port: 8200
    nodePort:
    annotations: { }
    staticIP:

  rollout:
    # The max surge in pods during a rollout
    maxSurge: 25%
    # The max unavailable during a rollout
    maxUnavailable: 25%

  resources:
    requests: { }
    #  cpu: 250m
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 60

  nodeSelector: { }

  tolerations: [ ]

  affinity: { }

  # -- Additional env variables to inject into the da server deployment.
  env: []


###
###
### ---- REDIS ----
###
###
redis:
  # -- Whether to deploy a Redis server to satisfy the applications requirements. To use an external redis instance set this to `false` and configure the `externalRedis` parameters.
  enabled: true

  nameOverride: "featbit-redis"

  fullnameOverride: ""

  architecture: standalone

  auth:
    # -- Enable Redis password authentication.
    enabled: false

    # -- Redis password.
    #    Defaults to a random 10-character alphanumeric string if not set.
    #    NOTE: ignored unless `redis.auth.enabled` is `true` or if `redis.auth.existingSecret` is set.
    #
    password: ""

    # -- The name of an existing secret containing the Redis credential to use.
    #    NOTE: ignored unless `redis.auth.enabled` is `true`.
    #          When it is set, the previous `redis.auth.password` parameter is ignored.
    #
    existingSecret: ""

    # -- Password key to be retrieved from existing secret.
    #    NOTE: ignored unless `redis.auth.existingSecret` parameter is set.
    #
    existingSecretPasswordKey: ""

  image:
    registry: docker.io

  master:
    persistence:
      # -- Enable data persistence using PVC.
      enabled: true

      # -- Persistent Volume size.
      size: 8Gi

externalRedis:
  sentinel:
    enabled: false
    # -- External Redis master set name
    masterSet: "mymaster"
  cluster:
    enabled: false
  # -- External Redis host:port to use.
  hosts: [ ]
  # -- External Redis database index, ignore in cluster mode
  db: 0
  # -- User for the external Redis server (for use with ACLs on redis 6 and above)
  user: ""
  # -- Password for the external Redis server. Ignored if `externalRedis.existingSecret` is set.
  password: ""
  # -- Name of an existing Kubernetes secret object containing the Redis password.
  existingSecret: ""
  # -- Name of the key pointing to the password in your Kubernetes secret.
  existingSecretPasswordKey: ""
  # -- Specifies that SSL encryption should be used
  ssl: false

###
###
### ---- MongoDB ----
###
###

mongodb:
  enabled: true

  nameOverride: "featbit-mongodb"
  fullnameOverride: ""

  extraScripts: featbit-mongodb-init-scripts-configmap

  settings:
    ## The root username
    rootUsername: "admin"
    ## The root user password
    rootPassword: "password"

  userDatabase:
    name: "featbit"
    user: "featbit"
    password: "featbit"

  storage:
    requestedSize: 8Gi

externalMongodb:
  fullConnectionString: ""
  # -- Name of an existing Kubernetes secret object containing the full conn string
  existingSecret: ""
  # -- Name of the key pointing to the full conn string in your Kubernetes secret.
  existingSecretKey: ""



busybox:
  image:
    registry: docker.io
    repository: busybox
    tag: 1.34
    pullPolicy: IfNotPresent
  imagePullSecrets: [ ]

kubectl:
  image:
    registry: docker.io
    repository: bitnami/kubectl
    tag: 1.26.6-debian-11-r31
    pullPolicy: IfNotPresent
  imagePullSecrets: [ ]



###
###
### ---- Variables Compoenents for FeatBit Pro----
###
###


isPro: false

kafka:
  enabled: false
  nameOverride: featbit-kafka
  extraConfig: "logRetentionHours=24;logRetentionBytes=_2_000_000_000"
  listeners:
    client:
      protocol: "PLAINTEXT"
    controller:
      protocol: "PLAINTEXT"
    interbroker:
      protocol: "PLAINTEXT"
    external:
      protocol: "PLAINTEXT"

  provisioning:
    enabled: true
    topics:
      - name: "featbit-feature-flag-change"
      - name: "featbit-segment-change"
      - name: "featbit-endusers"
      - name: "featbit-insights"

  controller:
    persistence:
      enabled: true
      size: 8Gi

externalKafka:
  # - External Kafka brokers. Ignored if `kafka.enabled` is set to `true`. Multiple brokers can be provided as array/list.
  brokers:
    producer:
      hosts: [ ]
      user: ""
      password: ""
      existingSecret: ""
      existingSecretPasswordKey: ""
      mechanism: "PLAIN"
      protocol: "PLAINTEXT"

    consumer:
      hosts: [ ]
      user: ""
      password: ""
      existingSecret: ""
      existingSecretPasswordKey: ""
      mechanism: "PLAIN"
      protocol: "PLAINTEXT"

clickhouse:
  enabled: false
  nameOverride: featbit-clickhouse
  shards: 1
  replicaCount: 1
  auth:
    username: "default"
    password: "featbit"
    existingSecret:	""
    existingSecretKey: ""
  keeper:
    enabled: true
  zookeeper:
    enabled: false
  persistence:
    enabled: true
    size: 8Gi

  # DO NOT MODIFY this value
  defaultConfigurationOverrides: |
    <clickhouse>
      <!-- Macros -->
      <macros>
        <shard from_env="CLICKHOUSE_SHARD_ID"></shard>
        <replica from_env="CLICKHOUSE_REPLICA_ID"></replica>
        <layer>{{ include "common.names.fullname" . }}</layer>
      </macros>
      <!-- Log Level -->
      <logger>
        <level>{{ .Values.logLevel }}</level>
      </logger>
      <!-- Cluster configuration - Any update of the shards and replicas requires helm upgrade -->
      <remote_servers>
        <featbit_ch_cluster>
          {{- $shards := $.Values.shards | int }}
          {{- range $shard, $e := until $shards }}
          <shard>
              {{- $replicas := $.Values.replicaCount | int }}
              {{- range $i, $_e := until $replicas }}
              <replica>
                  <host>{{ printf "%s-shard%d-%d.%s.%s.svc.%s" (include "common.names.fullname" $ ) $shard $i (include "clickhouse.headlessServiceName" $) (include "common.names.namespace" $) $.Values.clusterDomain }}</host>
                  <port>{{ $.Values.service.ports.tcp }}</port>
              </replica>
              {{- end }}
          </shard>
          {{- end }}
        </featbit_ch_cluster>
      </remote_servers>
      {{- if .Values.keeper.enabled }}
      <!-- keeper configuration -->
      <keeper_server>
        {{/*ClickHouse keeper configuration using the helm chart */}}
        <tcp_port>{{ $.Values.containerPorts.keeper }}</tcp_port>
        {{- if .Values.tls.enabled }}
        <tcp_port_secure>{{ $.Values.containerPorts.keeperSecure }}</tcp_port_secure>
        {{- end }}
        <server_id from_env="KEEPER_SERVER_ID"></server_id>
        <log_storage_path>/bitnami/clickhouse/keeper/coordination/log</log_storage_path>
        <snapshot_storage_path>/bitnami/clickhouse/keeper/coordination/snapshots</snapshot_storage_path>

        <coordination_settings>
            <operation_timeout_ms>10000</operation_timeout_ms>
            <session_timeout_ms>30000</session_timeout_ms>
            <raft_logs_level>trace</raft_logs_level>
        </coordination_settings>

        <raft_configuration>
        {{- $nodes := .Values.replicaCount | int }}
        {{- range $node, $e := until $nodes }}
        <server>
          <id>{{ $node | int }}</id>
          <hostname from_env="{{ printf "KEEPER_NODE_%d" $node }}"></hostname>
          <port>{{ $.Values.service.ports.keeperInter }}</port>
        </server>
        {{- end }}
        </raft_configuration>
      </keeper_server>
      {{- end }}
      {{- if or .Values.keeper.enabled .Values.zookeeper.enabled .Values.externalZookeeper.servers }}
      <!-- Zookeeper configuration -->
      <zookeeper>
        {{- if or .Values.keeper.enabled }}
        {{- $nodes := .Values.replicaCount | int }}
        {{- range $node, $e := until $nodes }}
        <node>
          <host from_env="{{ printf "KEEPER_NODE_%d" $node }}"></host>
          <port>{{ $.Values.service.ports.keeper }}</port>
        </node>
        {{- end }}
        {{- else if .Values.zookeeper.enabled }}
        {{/* Zookeeper configuration using the helm chart */}}
        {{- $nodes := .Values.zookeeper.replicaCount | int }}
        {{- range $node, $e := until $nodes }}
        <node>
          <host from_env="{{ printf "KEEPER_NODE_%d" $node }}"></host>
          <port>{{ $.Values.zookeeper.service.ports.client }}</port>
        </node>
        {{- end }}
        {{- else if .Values.externalZookeeper.servers }}
        {{/* Zookeeper configuration using an external instance */}}
        {{- range $node :=.Values.externalZookeeper.servers }}
        <node>
          <host>{{ $node }}</host>
          <port>{{ $.Values.externalZookeeper.port }}</port>
        </node>
        {{- end }}
        {{- end }}
      </zookeeper>
      {{- end }}
      {{- if .Values.tls.enabled }}
      <!-- TLS configuration -->
      <tcp_port_secure from_env="CLICKHOUSE_TCP_SECURE_PORT"></tcp_port_secure>
      <https_port from_env="CLICKHOUSE_HTTPS_PORT"></https_port>
      <openSSL>
          <server>
              {{- $certFileName := default "tls.crt" .Values.tls.certFilename }}
              {{- $keyFileName := default "tls.key" .Values.tls.certKeyFilename }}
              <certificateFile>/bitnami/clickhouse/certs/{{$certFileName}}</certificateFile>
              <privateKeyFile>/bitnami/clickhouse/certs/{{$keyFileName}}</privateKeyFile>
              <verificationMode>none</verificationMode>
              <cacheSessions>true</cacheSessions>
              <disableProtocols>sslv2,sslv3</disableProtocols>
              <preferServerCiphers>true</preferServerCiphers>
              {{- if or .Values.tls.autoGenerated .Values.tls.certCAFilename }}
              {{- $caFileName := default "ca.crt" .Values.tls.certCAFilename }}
              <caConfig>/bitnami/clickhouse/certs/{{$caFileName}}</caConfig>
              {{- else }}
              <loadDefaultCAFile>true</loadDefaultCAFile>
              {{- end }}
          </server>
          <client>
              <loadDefaultCAFile>true</loadDefaultCAFile>
              <cacheSessions>true</cacheSessions>
              <disableProtocols>sslv2,sslv3</disableProtocols>
              <preferServerCiphers>true</preferServerCiphers>
              <verificationMode>none</verificationMode>
              <invalidCertificateHandler>
                  <name>AcceptCertificateHandler</name>
              </invalidCertificateHandler>
          </client>
      </openSSL>
      {{- end }}
      {{- if .Values.metrics.enabled }}
       <!-- Prometheus metrics -->
       <prometheus>
          <endpoint>/metrics</endpoint>
          <port from_env="CLICKHOUSE_METRICS_PORT"></port>
          <metrics>true</metrics>
          <events>true</events>
          <asynchronous_metrics>true</asynchronous_metrics>
      </prometheus>
      {{- end }}
      <!-- Maximum number of concurrent queries. -->
      <max_concurrent_queries>1000</max_concurrent_queries>
    </clickhouse>
externalClickhouse:
  # -- Host of the external clickhouse. This is required when clickhouse.enabled is false
  host: ""
  # -- TCP(security) port of the external clickhouse. This is required when clickhouse.enabled is false
  tcpPort: 9000
  # -- HTTP(s) port of the external clickhouse. This is required when clickhouse.enabled is false
  httpPort: 8123
  # -- if the external clickhouse has more than one shard, set a list of clickhouse extra shards in host:tcp.
  altHosts: [ ]
  # -- Name of the external cluster to run DDL queries on. This is required when clickhouse.enabled is false
  cluster: ""
  # -- Database name for the external cluster
  database: "featbit"
  # -- User name for the external cluster to connect to the external cluster as
  user: "default"
  # -- Password for the cluster. Ignored if existingClickhouse.existingSecret is set
  password: ""
  # -- Name of an existing Kubernetes secret object containing the password
  existingSecret: ""
  # -- Name of the key pointing to the password in your Kubernetes secret
  existingSecretKey: ""
  # -- Whether to use TLS connection connecting to ClickHouse
  secure: false
  # -- Whether to verify TLS connection connecting to ClickHouse
  verify: false

openTelemetry:
  enabled: false
  endpoint: ""
  protocol: "grpc"
  insecure: true
  timeoutInMilliseconds: 10000
